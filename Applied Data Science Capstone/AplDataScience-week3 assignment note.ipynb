{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will be required to explore, segment, and cluster the neighborhoods in the city of Toronto. However, unlike New York, the neighborhood data is not readily available on the internet. What is interesting about the field of data science is that each project can be challenging in its unique way, so you need to learn to be agile and refine the skill to learn new libraries and tools quickly depending on the project.\n",
    "\n",
    "For the Toronto neighborhood data, a Wikipedia page exists that has all the information we need to explore and cluster the neighborhoods in Toronto. You will be required to scrape the Wikipedia page and wrangle the data, clean it, and then read it into a pandas dataframe so that it is in a structured format like the New York dataset.\n",
    "\n",
    "Once the data is in a structured format, you can replicate the analysis that we did to the New York City dataset to explore and cluster the neighborhoods in the city of Toronto.\n",
    "\n",
    "Your submission will be a link to your Jupyter Notebook on your Github repository.\n",
    "\n",
    "This assignment will be graded by other peers who are completing this course during the same session. This assignment is worth 15% of your total grade.\n",
    "\n",
    "You can prepare one notebook for all three parts, but please use Markdown to clearly label your work for each part in order to make it easy for your peers to grade your work. However, you will have to submit the notebook three times since a submission has to be associated with each question. Sorry about that.\n",
    "\n",
    "Please check the My Submission section before you start working on the assignment.\n",
    "\n",
    "For this assignment, you will be required to explore and cluster the neighborhoods in Toronto.\n",
    "\n",
    "Start by creating a new Notebook for this assignment.\n",
    "Use the Notebook to build the code to scrape the following Wikipedia page, https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M, in order to obtain the data that is in the table of postal codes and to transform the data into a pandas dataframe like the one shown below:\n",
    "\n",
    "3. To create the above dataframe:\n",
    "\n",
    "The dataframe will consist of three columns: PostalCode, Borough, and Neighborhood\n",
    "Only process the cells that have an assigned borough. Ignore cells with a borough that is Not assigned.\n",
    "More than one neighborhood can exist in one postal code area. For example, in the table on the Wikipedia page, you will notice that M5A is listed twice and has two neighborhoods: Harbourfront and Regent Park. These two rows will be combined into one row with the neighborhoods separated with a comma as shown in row 11 in the above table.\n",
    "If a cell has a borough but a Not assigned neighborhood, then the neighborhood will be the same as the borough.\n",
    "Clean your Notebook and add Markdown cells to explain your work and any assumptions you are making.\n",
    "In the last cell of your notebook, use the .shape method to print the number of rows of your dataframe.\n",
    "4. Submit a link to your Notebook on your Github repository. (10 marks)\n",
    "\n",
    "Note: There are different website scraping libraries and packages in Python. For scraping the above table, you can simply use pandas to read the table into a pandas dataframe.\n",
    "\n",
    "Another way, which would help to learn for more complicated cases of web scraping is using the BeautifulSoup package. Here is the package's main documentation page: http://beautiful-soup-4.readthedocs.io/en/latest/\n",
    "\n",
    "The package is so popular that there is a plethora of tutorials and examples on how to use it. Here is a very good Youtube video on how to use the BeautifulSoup package: https://www.youtube.com/watch?v=ng2o98k983k\n",
    "\n",
    "Use pandas, or the BeautifulSoup package, or any other way you are comfortable with to transform the data in the table on the Wikipedia page into the above pandas dataframe.\n",
    "\n",
    "Now that you have built a dataframe of the postal code of each neighborhood along with the borough name and neighborhood name, in order to utilize the Foursquare location data, we need to get the latitude and the longitude coordinates of each neighborhood.\n",
    "\n",
    "In an older version of this course, we were leveraging the Google Maps Geocoding API to get the latitude and the longitude coordinates of each neighborhood. However, recently Google started charging for their API: http://geoawesomeness.com/developers-up-in-arms-over-google-maps-api-insane-price-hike/, so we will use the Geocoder Python package instead: https://geocoder.readthedocs.io/index.html.\n",
    "\n",
    "The problem with this Package is you have to be persistent sometimes in order to get the geographical coordinates of a given postal code. So you can make a call to get the latitude and longitude coordinates of a given postal code and the result would be None, and then make the call again and you would get the coordinates. So, in order to make sure that you get the coordinates for all of our neighborhoods, you can run a while loop for each postal code. Taking postal code M5G as an example, your code would look something like this\n",
    "\n",
    "Important Note: There is a limit on how many times you can call geocoder.google function. It is 2500 times per day. This should be way more than enough for you to get acquainted with the package and to use it to get the geographical coordinates of the neighborhoods in the Toronto.\n",
    "\n",
    "Once you are able to create the above dataframe, submit a link to the new Notebook on your Github repository. (2 marks)\n",
    "xplore and cluster the neighborhoods in Toronto. You can decide to work with only boroughs that contain the word Toronto and then replicate the same analysis we did to the New York City data. It is up to you.\n",
    "\n",
    "Just make sure:\n",
    "\n",
    "to add enough Markdown cells to explain what you decided to do and to report any observations you make.\n",
    "to generate maps to visualize your neighborhoods and how they cluster together.\n",
    "Once you are happy with your analysis, submit a link to the new Notebook on your Github repository. (3 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/lyllme/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - urllib3\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2019.8.28  |                0         132 KB  anaconda\n",
      "    certifi-2019.9.11          |           py37_0         154 KB  anaconda\n",
      "    conda-4.8.2                |           py37_0         3.0 MB  anaconda\n",
      "    openssl-1.1.1d             |       h7b6447c_2         3.7 MB  anaconda\n",
      "    urllib3-1.24.2             |           py37_0         153 KB  anaconda\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         7.2 MB\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  ca-certificates                                 pkgs/main --> anaconda\n",
      "  certifi                                         pkgs/main --> anaconda\n",
      "  conda                                           pkgs/main --> anaconda\n",
      "  openssl                                         pkgs/main --> anaconda\n",
      "  urllib3                                         pkgs/main --> anaconda\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "urllib3-1.24.2       | 153 KB    | ##################################### | 100% \n",
      "openssl-1.1.1d       | 3.7 MB    | ##################################### | 100% \n",
      "certifi-2019.9.11    | 154 KB    | ##################################### | 100% \n",
      "conda-4.8.2          | 3.0 MB    | ##################################### | 100% \n",
      "ca-certificates-2019 | 132 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install beautifulsoup4 --yes\n",
    "!conda install -c anaconda urllib3 --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of postal codes of Canada: M - Wikipedia\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "import certifi\n",
    "\n",
    "# アクセスするURL\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M\"\n",
    "\n",
    "# httpsの証明書検証を実行している\n",
    "http = urllib3.PoolManager(\n",
    "    cert_reqs='CERT_REQUIRED',\n",
    "    ca_certs=certifi.where())\n",
    "r = http.request('GET', url)\n",
    "\n",
    "soup = BeautifulSoup(r.data, 'html.parser')\n",
    "\n",
    "# タイトル要素を取得する → <title>経済、株価、ビジネス、政治のニュース:日経電子版</title>\n",
    "title_tag = soup.title\n",
    "\n",
    "# 要素の文字列を取得する → 経済、株価、ビジネス、政治のニュース:日経電子版\n",
    "title = title_tag.string\n",
    "\n",
    "# タイトル要素を出力\n",
    "# print(title_tag)\n",
    "\n",
    "# タイトルを文字列を出力\n",
    "print(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 終了時、同じディレクトリ上にcsvが発行される\n",
    "import csv\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# URLの指定\n",
    "html = urlopen(\"https://www.oreilly.co.jp/ebook/\")\n",
    "bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# テーブルを指定\n",
    "table = bsObj.findAll(\"table\", {\"class\":\"tablesorter\"})[0]\n",
    "rows = table.findAll(\"tr\")\n",
    "\n",
    "with open(\"ebooks.csv\", \"w\", encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for row in rows:\n",
    "        csvRow = []\n",
    "        for cell in row.findAll(['td', 'th']):\n",
    "            csvRow.append(cell.get_text())\n",
    "        writer.writerow(csvRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f30dfe22c429>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# テーブルを指定\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbsObj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"table\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"wikitable sortable jquery-tablesorter\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# 終了時、同じディレクトリ上にcsvが発行される\n",
    "import csv\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# URLの指定\n",
    "html = urlopen(\"https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M\")\n",
    "bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# テーブルを指定\n",
    "table = bsObj.findAll(\"table\", {\"class\":\"wikitable sortable jquery-tablesorter\"})[0]\n",
    "rows = table.findAll(\"tr\")\n",
    "\n",
    "with open(\"canada.csv\", \"w\", encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for row in rows:\n",
    "        csvRow = []\n",
    "        for cell in row.findAll(['td', 'th']):\n",
    "            csvRow.append(cell.get_text())\n",
    "        writer.writerow(csvRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
